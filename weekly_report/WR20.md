Progress:
* First, I want to apologize for a serious mistake I made. The execution bottleneck I found before exists in calculating the q/k/v matrices of all the heads. The issue arises because we compute the matrix for each head individually, which leads to low utilization due to the small size of the second matrix operand(768x64). But this week I revisited other PyTorch implementations, and one common way to do this is to compute the q/k/v matrices of all the heads together using only one matrix multiplication (we obtain the q matrices of all the heads horizontally concatenated). In this case, the bottleneck no longer exists because the size of the second matrix operand is 768x768, and it can be efficiently executed by the vanilla matmul and hardware. The updated results show that my bc_matmul can only outperform the vanilla by 3% for 4 lanes and by 11% for 16 lanes. To summarize, the new algorithm and hardware are not a big improvement for large matrices (since there is not much room for improvement, the utilization is already very high), but have significant advantages for small size and narrow shape matrices. demo
* Verified the model I wrote with the annotated transformer model from Harvard.
* Still having some problems with the gate-level simulation. The backend is finished, and the WNS is 58ps (close to the baseline).

Before the end of the holiday:
* Finish power testing.
* Write a matmul benchmark for different sizes and shapes.
* Collate all the results.
* Write the introduction & background chapters.
