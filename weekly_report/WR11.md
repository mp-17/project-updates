Progress:
* Refined the new MatMul algorithm, which can improve data reuse several times by slicing
* Tested the effect of different model sizes and sequence lengths on performance. Model size has no performance impact on all kernels except matmul; softmax and layernorm should have gained performance at longer sequences, but the gain is negligible because stride load takes too long.
* Proposed a solution for memory bandwidth. When using a multi-ara architecture, the memory bandwidth will need to increase proportionally with the number of lanes. My solution is to use a small memory bandwidth and provide data to only one ara at a time. Because MatMul can take full advantage of data reuse, the small bandwidth is sufficient.

Next Week:
* Try to find a solution to increase the issue rate of the vector dispatcher in CVA6. Based on last week's results, an ideal dispatcher (1 inst/cycle) can improve the MatMul performance 3x.
* Implement the broadcast datapath.
