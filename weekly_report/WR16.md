Progress:
* Finished verification and also optimized both the hardware and software. The biggest improvement of the algorithm is to avoid strided memory operations (very inefficient) by storing the A matrix in transposed form. Then I can use burst mode to load and store data. In addition, transpose A also accelerates the execution of LayerNorm and Softmax and has little effect on ReLU and Dropout. I also use different registers to resolve false data dependency.
* Tested the MatMul in self-attention (results, the Ara used for testing does not yet adopt the burst store described above). The utilization of 16 lanes can be 4x higher than the vanilla MatMul.

Next Week:
* Although the test results are already satisfactory, I think there are still two points that can be optimized: 1. complete the above burst store, and 2. After the first lane is completed, it can directly execute the next instruction instead of stalling and waiting for the last lane. (The more lanes we have, the longer the stall time, this also explains why we have decreasing utilization). It is possible to push the utilization to close to 100% after completing these two optimizations.
* Meanwhile, I will start the backend flow to do a preliminary check. (make sure no new critical path)
