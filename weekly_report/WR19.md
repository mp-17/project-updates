Progress:
* Finished modifying the transformer model to process transposed matrix. One of the most major changes is that the output of the self-attention layer should be calculated as mat_o = mat_a * mat_b, but since both mat_a and mat_b are transposed, we need to use the transpose property of matrix multiplication: mat_o^T = mat_b^T * mat_a^T. The performance of dropout and relu is not affected at all by the transpose matrix, because they can flatten the input as a one-dimensional vector.
* Still running the backend (poor server). But based on the available results, the optimization method is quite successful.

Next Week:
* After the backend is completed, test the power. (GOPs/W)
* As Victor suggested, I will use a pretrained PyTorch transformer model as the golden model for verification. (Currently the golden model is written by myself layer by layer.)
