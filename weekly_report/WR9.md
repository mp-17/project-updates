Progress:
* Previously I only simulated the transformer kernels with 4 lanes; this week I also simulated with 8 and 16 lanes.
* The results show that most of the execution time is consumed by MatMul (>80%), and if the number of lanes is doubled, the FPU utilization of matmul is reduced by half. In other words, increasing the number of lanes does not reduce the execution time. The reason for this is due to the short length of the vector and the fact that the instructions sent by Ariane to Ara are not consecutive, with more than ten cycles of gaps between adjacent instructions. I have two solutions in mind, one is at the algorithmic level: increase the vector length by merging multiple vectors, so that the next MAC instruction will have arrived at the FPU before the previous MAC instruction has finished. However, this method cannot solve the problem completely. If we want to achieve the performance of Eyeriss V2 (~200 GFLOPS), we need to use 64 lanes, and by then there is no way to efficiently use the FPU even with the maximum vector length. The second way I came up with is to add a data path from the L2 cache to each lane. (we can use a 2D mesh structure to avoid critical paths) Whenever the load unit reads a vector, this vector will be broadcast to each lane so that each lane has enough data regardless of the number of lanes. (need to slightly change the algorithm)

Next Week:
* Discuss the bottleneck and my solution with my supervisors, and if we decide to optimize the architecture, start the design work on paper.
